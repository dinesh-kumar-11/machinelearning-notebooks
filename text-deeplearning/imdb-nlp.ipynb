{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrive the data in the followin dir - http://mng.bz/0tIo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_dir = './aclImdb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.path.join(imdb_dir, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels =[]\n",
    "texts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-0e978e5a8936>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'.txt'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m             \u001b[0mtexts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mypy3/lib/python3.8/_bootlocale.py\u001b[0m in \u001b[0;36mgetpreferredencoding\u001b[0;34m(do_setlocale)\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mlocale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpreferredencoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo_setlocale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0;32mdef\u001b[0m \u001b[0mgetpreferredencoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo_setlocale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdo_setlocale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutf8_mode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for label_type in ['neg','pos']:\n",
    "    dir_name = os.path.join(train_dir, label_type)\n",
    "    for file in os.listdir(dir_name):\n",
    "        if file[-4:] == '.txt':\n",
    "            f = open(os.path.join(dir_name, file))\n",
    "            texts.append(f.read())\n",
    "            f.close()\n",
    "            if label_type == 'neg':\n",
    "                labels.append(0)\n",
    "            else:\n",
    "                labels.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 100\n",
    "training_samples = 100\n",
    "validation_samples = 100\n",
    "max_words = 10000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words = max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the sequence type is <class 'list'>  and the seq len is 25000\n"
     ]
    }
   ],
   "source": [
    "print('the sequence type is', type(sequences), ' and the seq len is', len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 88582 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pad_sequences(sequences, maxlen=maxlen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the data type is <class 'numpy.ndarray'>  and the data len is 25000\n"
     ]
    }
   ],
   "source": [
    "print('the data type is', type(data), ' and the data len is', len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (25000, 100)\n",
      "Shape of label tensor: (25000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "labels = np.asarray(labels)\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = np.arange(data.shape[0])\n",
    "indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 100)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = data[:training_samples]\n",
    "y_train = labels[:training_samples]\n",
    "x_val = data[training_samples: training_samples + validation_samples]\n",
    "y_val = labels[training_samples: training_samples + validation_samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading the GloVe word embeddings\n",
    "Go to https://nlp.stanford.edu/projects/glove, and download the precomputed embeddings from 2014 English Wikipedia. It’s an 822 MB zip file called glove.6B.zip, containing 100-dimensional embedding vectors for 400,000 words (or nonword tokens). Unzip it.\n",
    "\n",
    "## Preprocessing the embeddings\n",
    "Let’s parse the unzipped file (a .txt file) to build an index that maps words (as strings) to their vector representation (as number vectors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_dir = './glove.6B'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings_index['king'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you’ll build an embedding matrix that you can load into an Embedding layer. It must be a matrix of shape (max_words, embedding_dim), where each entry i contains the embedding_dim-dimensional vector for the word of index i in the reference word index (built during tokenization). Note that index 0 isn’t supposed to stand for any word or token—it’s a placeholder.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the dimension of the embedding from glove.6B.100d.txt\n",
    "embedding_dim = 100\n",
    "\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i < max_words:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 100)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 100)          1000000   \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 10000)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                320032    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,320,065\n",
      "Trainable params: 1,320,065\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Embedding layer has a single weight matrix: a 2D float matrix where each entry i is the word vector meant to be associated with index i. Simple enough. Load the GloVe matrix you prepared into the Embedding layer, the first layer in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the co-relation between the words are already defined in geometric spac, we are re-using the same matrix weights, it need not be trainied again\n",
    "model.layers[0].set_weights([embedding_matrix])\n",
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, you’ll freeze the Embedding layer (set its trainable attribute to False), following the same rationale you’re already familiar with in the context of pretrained convnet features: when parts of a model are pretrained (like your Embedding layer) and parts are randomly initialized (like your classifier), the pretrained parts shouldn’t be updated during training, to avoid forgetting what they already know. The large gradient updates triggered by the randomly initialized layers would be disruptive to the already-learned features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', metrics=['acc'], loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 1.0462 - acc: 0.4300 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.3511 - acc: 0.8200 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.1761 - acc: 0.9700 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0868 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0349 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0188 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0110 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0083 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0060 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0047 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, epochs=10, batch_size=20, validation_data=[x_val, y_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('imdb_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "epocs = range(1, len(history.history['acc'])+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1534d2c40>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epocs, history.history['acc'], 'bo', label='Train acc')\n",
    "plt.plot(epocs, history.history['val_acc'], 'b', label='val acc')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
